{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27faca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from transformers import AutoConfig, AutoModelForCausalLM\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a09dde",
   "metadata": {},
   "source": [
    "### GPT-2\n",
    "\n",
    "From https://huggingface.co/docs/transformers/en/model_doc/gpt2#transformers.GPT2Config we get the following information about GPT-2:\n",
    "\n",
    "(vocab_size = 50257, n_positions = 1024, n_embd = 768, n_layer = 12, n_head = 12, ...).\n",
    "\n",
    "**We can use this information to define the parameters.**\n",
    "\n",
    "**Note that ideally we need to load the model and extract parameters from there. But I decided to make it less time consuming, and just found the information about the parameters (moreover, it might take a lot of time and memory to load all these models).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "311efed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_per_param = 2 # 16 bits = 2 bytes\n",
    "\n",
    "L = 1024\n",
    "V = 50257\n",
    "d = 768\n",
    "N_layer = 12\n",
    "d_ff = 3072 # Found in the internet, 768 * 4\n",
    "n_head = 12\n",
    "head_dim = 64 # d_gpt2 / n_head_gpt2\n",
    "\n",
    "\n",
    "\n",
    "n_h = 12\n",
    "n_kv = 4    # For GQA\n",
    "d_ff = 3072 # FFN intermediate size (GPT-2)\n",
    "\n",
    "\n",
    "E = 2       # Expansion ratio (Mamba-2)\n",
    "N = 128     # State size (Mamba-2)\n",
    "d_k = 64    # Key dimension (GLA)\n",
    "d_v = 64    # Value dimension (GLA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a891ece3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  124,318,464\n",
      "Model size (bfloat16): 237.1 MiB\n"
     ]
    }
   ],
   "source": [
    "# We will ignore small terms such as LayerNorms, biases, etc. Even though it will result in a tiny fraction of the parameters, but it will simplify the calculations.\n",
    "\n",
    "# Embeddings + position embeddings\n",
    "param_embed = V * d\n",
    "param_pos_embed = L * d\n",
    "\n",
    "# Each layer has:\n",
    "#   - QKV projection: 3 x (d x d)\n",
    "#   - output projection: (d x d)\n",
    "#   - FFN: first weight (d x d_ff)\n",
    "#          second weight (d_ff x d)\n",
    "param_per_layer = 3 * d * d + 1 * d * d + d * d_ff + d_ff * d\n",
    "# total parameters\n",
    "param_total = param_embed + param_pos_embed + N_layer * param_per_layer\n",
    "# convert to MiB\n",
    "size_bytes = param_total * bytes_per_param\n",
    "size_MiB   = size_bytes / (1024**2)\n",
    "\n",
    "print(f\"Total parameters:  {param_total:,.0f}\")\n",
    "print(f\"Model size (bfloat16): {size_MiB:,.1f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69d06c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KV cache elements: 18,874,368\n",
      "KV cache size (bfloat16): 36.0 MiB\n"
     ]
    }
   ],
   "source": [
    "# since the batch size is 1 on inference, the number of cache elements either for k or v per layer is number of attention heads times the head dimension times the sequence length\n",
    "# then we multiply it by the number of layers and 2 (making sure we calculate the elements both for k and v)\n",
    "kv_elements = 2 * N_layer * n_head * L * head_dim\n",
    "kv_bytes = kv_elements * bytes_per_param\n",
    "kv_MiB = kv_bytes / (1024**2)\n",
    "\n",
    "print(f\"KV cache elements: {kv_elements:,}\")\n",
    "print(f\"KV cache size (bfloat16): {kv_MiB:,.1f} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48dc2bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs per layer: 17.7 GFLOPs\n",
      "LM-head FLOPs: 79.0 GFLOPs\n",
      "Total FLOPs: 291.6 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "flops_per_layer = (\n",
    "    6 * L * d**2        # QKV proj\n",
    "  + 4 * d * L**2        # attention core\n",
    "  + 2 * L * d**2        # output proj\n",
    "  + 4 * L * d * d_ff    # FFN\n",
    ")\n",
    "\n",
    "# LM-head (tied embeddings): final logits = 2·L·d·V\n",
    "flops_lm_head = 2 * L * d * V\n",
    "\n",
    "# total\n",
    "total_flops = N_layer * flops_per_layer + flops_lm_head\n",
    "\n",
    "# convert to GFLOPs\n",
    "gflops = total_flops / 1e9\n",
    "\n",
    "print(f\"FLOPs per layer: {flops_per_layer/1e9:.1f} GFLOPs\")\n",
    "print(f\"LM-head FLOPs: {flops_lm_head/1e9:.1f} GFLOPs\")\n",
    "print(f\"Total FLOPs: {gflops:.1f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "297ec3d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291648307200"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_flops\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07665802",
   "metadata": {},
   "source": [
    "### GPT-2 with GQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe49d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Total Parameters</th>\n",
       "      <th>Model Size (MiB)</th>\n",
       "      <th>KV-Cache (MiB)</th>\n",
       "      <th>FLOPs per Forward (GFLOPs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-2 Small</td>\n",
       "      <td>124318464</td>\n",
       "      <td>237.118652</td>\n",
       "      <td>36.0</td>\n",
       "      <td>291.648307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-2 Small + GQA (G=4)</td>\n",
       "      <td>114881280</td>\n",
       "      <td>219.118652</td>\n",
       "      <td>12.0</td>\n",
       "      <td>272.320954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Total Parameters  Model Size (MiB)  \\\n",
       "0              GPT-2 Small         124318464        237.118652   \n",
       "1  GPT-2 Small + GQA (G=4)         114881280        219.118652   \n",
       "\n",
       "   KV-Cache (MiB)  FLOPs per Forward (GFLOPs)  \n",
       "0            36.0                  291.648307  \n",
       "1            12.0                  272.320954  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = 4 # number of KV groups in GQA\n",
    "\n",
    "embed_params = V * d\n",
    "pos_emb_params = L * d\n",
    "\n",
    "# GQA GPT-2 model size\n",
    "q_params = d * d\n",
    "kv_params_gqa = 2 * d * (G * head_dim)\n",
    "o_params = d * d\n",
    "ffn_params = d * d_ff + d_ff * d\n",
    "layer_params_gqa = q_params + kv_params_gqa + o_params + ffn_params\n",
    "total_params_gqa = embed_params + pos_emb_params + N_layer * layer_params_gqa\n",
    "model_size_gqa = total_params_gqa * bytes_per_param\n",
    "model_size_gqa_mib = model_size_gqa / (2**20)\n",
    "\n",
    "# KV-Cache Size (bytes → MiB)\n",
    "kv_cache_gqa = L * N_layer * G * head_dim * 2 * bytes_per_param\n",
    "kv_cache_gqa_mib = kv_cache_gqa / (2**20)\n",
    "\n",
    "# FLOPs per Forward Pass (→ GFLOPs)\n",
    "attn_score_flops = 2 * n_head * L * L * head_dim\n",
    "attn_apply_flops = 2 * n_head * L * L * head_dim\n",
    "ffn_flops = 2 * L * d * d_ff + 2 * L * d_ff * d\n",
    "# GQA projections: Q,O full; K,V reduced\n",
    "proj_flops_gqa = (2 * L * d * d) + 2 * (2 * L * d * (G * head_dim)) + (2 * L * d * d)\n",
    "flops_per_layer_gqa = proj_flops_gqa + attn_score_flops + attn_apply_flops + ffn_flops\n",
    "total_flops_gqa = N_layer * flops_per_layer_gqa + 2 * L * d * V\n",
    "total_flops_gqa_g = total_flops_gqa / 1e9\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    \"Model\": [\"GPT-2 Small\", f\"GPT-2 Small + GQA (G={G})\"],\n",
    "    \"Total Parameters\": [param_total, total_params_gqa],\n",
    "    \"Model Size (MiB)\": [size_MiB, model_size_gqa_mib],\n",
    "    \"KV-Cache (MiB)\": [kv_MiB, kv_cache_gqa_mib],\n",
    "    \"FLOPs per Forward (GFLOPs)\": [gflops, total_flops_gqa_g]\n",
    "})\n",
    "\n",
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa48fa2",
   "metadata": {},
   "source": [
    "### Mamba-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2fc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 95,220,480\n",
      "Model size: 181.62 MiB\n",
      "KV-cache size: 36.00 MiB\n",
      "FLOPs per forward: 154.6 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "L = 1024\n",
    "V = 50257\n",
    "d = 768\n",
    "N_layer = 12\n",
    "E = 2\n",
    "bfloat16_bytes = 2\n",
    "\n",
    "# Model Size (parameters and MiB)\n",
    "# P_total = V*d + N_layer * [3*d*d (QKV) + d*d (output) + 2*(d * (E*d)) (MLP)]\n",
    "param_count = V*d + N_layer*(4*d*d + 2*d*(E*d))\n",
    "model_size_bytes = param_count * bfloat16_bytes\n",
    "model_size_mib = model_size_bytes / (2**20)\n",
    "\n",
    "# KV‑Cache Size per sequence (MiB)\n",
    "# KV elements = N_layer * L * 2 * d\n",
    "kv_elements = N_layer * L * 2 * d\n",
    "kv_bytes = kv_elements * bfloat16_bytes\n",
    "kv_mib = kv_bytes / (2**20)\n",
    "\n",
    "# FLOPs per forward (GFLOPs)\n",
    "# Attention: 8*L*d^2 + 4*L^2*d per layer\n",
    "# MLP: 4*E*L*d^2 per layer\n",
    "attn_flops_per_layer = 8 * L * d**2 + 4 * L**2 * d\n",
    "mlp_flops_per_layer = 4 * E * L * d**2\n",
    "total_flops = N_layer * (attn_flops_per_layer + mlp_flops_per_layer)\n",
    "total_gflops = total_flops / 1e9\n",
    "\n",
    "comparison_new_row = ['Mamba-2', param_count, model_size_mib, kv_mib, total_gflops]\n",
    "comparison = pd.concat([comparison, pd.DataFrame([comparison_new_row], columns=comparison.columns)], ignore_index=True)\n",
    "\n",
    "print(\"Total parameters: {:,}\".format(param_count))\n",
    "print(\"Model size: {:.2f} MiB\".format(model_size_mib))\n",
    "print(\"KV-cache size: {:.2f} MiB\".format(kv_mib))\n",
    "print(\"FLOPs per forward: {:.1f} GFLOPs\".format(total_gflops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777d79a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Total Parameters</th>\n",
       "      <th>Model Size (MiB)</th>\n",
       "      <th>KV-Cache (MiB)</th>\n",
       "      <th>FLOPs per Forward (GFLOPs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-2 Small</td>\n",
       "      <td>124318464</td>\n",
       "      <td>237.118652</td>\n",
       "      <td>36.0</td>\n",
       "      <td>291.648307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-2 Small + GQA (G=4)</td>\n",
       "      <td>114881280</td>\n",
       "      <td>219.118652</td>\n",
       "      <td>12.0</td>\n",
       "      <td>272.320954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mamba-2</td>\n",
       "      <td>95220480</td>\n",
       "      <td>181.618652</td>\n",
       "      <td>36.0</td>\n",
       "      <td>154.618823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Total Parameters  Model Size (MiB)  \\\n",
       "0              GPT-2 Small         124318464        237.118652   \n",
       "1  GPT-2 Small + GQA (G=4)         114881280        219.118652   \n",
       "2                  Mamba-2          95220480        181.618652   \n",
       "\n",
       "   KV-Cache (MiB)  FLOPs per Forward (GFLOPs)  \n",
       "0            36.0                  291.648307  \n",
       "1            12.0                  272.320954  \n",
       "2            36.0                  154.618823  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0398c7b",
   "metadata": {},
   "source": [
    "### GLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bbb93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 123,532,032 (235.6 MiB)\n",
      "KV cache size: 36.0 MiB\n",
      "FLOPs per forward pass: 177.2 GFLOPs\n"
     ]
    }
   ],
   "source": [
    "d = 768\n",
    "N_layer = 12\n",
    "n_h = 12\n",
    "d_k = d // n_h\n",
    "d_v = d_k\n",
    "d_ff = 4 * d\n",
    "V = 50257\n",
    "L = 1024\n",
    "\n",
    "# Model Size: count parameters\n",
    "# Embedding + LM head (weight tying cancels doubling?)\n",
    "emb_params = V * d\n",
    "# Per-layer parameters:\n",
    "# Attention: QKV proj (3*d*d) + output proj (d*d)\n",
    "attn_params = 3 * d * d + d * d\n",
    "# Feed-forward: d*d_ff (in) + d_ff*d (out)\n",
    "ffn_params = d * d_ff + d_ff * d\n",
    "# Total per layer\n",
    "per_layer_params = attn_params + ffn_params\n",
    "# Total parameters\n",
    "total_params = emb_params + per_layer_params * N_layer\n",
    "# Bytes in bfloat16\n",
    "bytes_per_param = 2\n",
    "model_size_bytes = total_params * bytes_per_param\n",
    "model_size_mib = model_size_bytes / (1024**2)\n",
    "\n",
    "# KV Cache Size: keys + values for all layers and heads\n",
    "def calc_kv_cache_bytes(L, N_layer, n_h, d_k, d_v):\n",
    "    # total elements: N_layer * n_h * L * (d_k + d_v)\n",
    "    elements = N_layer * n_h * L * (d_k + d_v)\n",
    "    return elements * bytes_per_param\n",
    "\n",
    "total_kv_bytes = calc_kv_cache_bytes(L, N_layer, n_h, d_k, d_v)\n",
    "total_kv_mib = total_kv_bytes / (1024**2)\n",
    "\n",
    "# FLOPs per forward pass\n",
    "def matmul_flops(a, b, c):\n",
    "    return 2 * a * b * c\n",
    "\n",
    "# Attention FLOPs per layer:\n",
    "# QKV projections: 3 * (L*d*d)\n",
    "qkv_flops = 3 * matmul_flops(L, d, d)\n",
    "# Q*K^T: L * (d_k) * L\n",
    "qk_flops = matmul_flops(L, d_k, L)\n",
    "# attn weights * V: L * L * d_v\n",
    "attn_v_flops = matmul_flops(L, L, d_v)\n",
    "# output projection: L * d * d\n",
    "proj_flops = matmul_flops(L, d, d)\n",
    "# Feed-forward FLOPs per layer: input and output\n",
    "ffn_fflops = matmul_flops(L, d, d_ff) + matmul_flops(L, d_ff, d)\n",
    "# Total per layer\n",
    "per_layer_flops = qkv_flops + qk_flops + attn_v_flops + proj_flops + ffn_fflops\n",
    "# Total FLOPs\n",
    "total_flops = per_layer_flops * N_layer\n",
    "# Convert to GFLOPs\n",
    "total_gflops = total_flops / 1e9\n",
    "\n",
    "comparison_new_row = ['GLA', total_params, model_size_mib, total_kv_mib, total_gflops]\n",
    "comparison = pd.concat([comparison, pd.DataFrame([comparison_new_row], columns=comparison.columns)], ignore_index=True)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,d} ({model_size_mib:.1f} MiB)\")\n",
    "print(f\"KV cache size: {total_kv_mib:.1f} MiB\")\n",
    "print(f\"FLOPs per forward pass: {total_gflops:.1f} GFLOPs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f3a6b",
   "metadata": {},
   "source": [
    "**print the table summarizing all the results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a92948f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Total Parameters</th>\n",
       "      <th>Model Size (MiB)</th>\n",
       "      <th>KV-Cache (MiB)</th>\n",
       "      <th>FLOPs per Forward (GFLOPs)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-2 Small</td>\n",
       "      <td>124318464</td>\n",
       "      <td>237.118652</td>\n",
       "      <td>36.0</td>\n",
       "      <td>291.648307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPT-2 Small + GQA (G=4)</td>\n",
       "      <td>114881280</td>\n",
       "      <td>219.118652</td>\n",
       "      <td>12.0</td>\n",
       "      <td>272.320954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mamba-2</td>\n",
       "      <td>95220480</td>\n",
       "      <td>181.618652</td>\n",
       "      <td>36.0</td>\n",
       "      <td>154.618823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GLA</td>\n",
       "      <td>123532032</td>\n",
       "      <td>235.618652</td>\n",
       "      <td>36.0</td>\n",
       "      <td>177.167401</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Model  Total Parameters  Model Size (MiB)  \\\n",
       "0              GPT-2 Small         124318464        237.118652   \n",
       "1  GPT-2 Small + GQA (G=4)         114881280        219.118652   \n",
       "2                  Mamba-2          95220480        181.618652   \n",
       "3                      GLA         123532032        235.618652   \n",
       "\n",
       "   KV-Cache (MiB)  FLOPs per Forward (GFLOPs)  \n",
       "0            36.0                  291.648307  \n",
       "1            12.0                  272.320954  \n",
       "2            36.0                  154.618823  \n",
       "3            36.0                  177.167401  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf90937",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
