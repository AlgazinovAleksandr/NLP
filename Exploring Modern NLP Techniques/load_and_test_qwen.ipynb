{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14f525a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! git clone https://github.com/huggingface/transformers.git   # clones the needed repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f96ad1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe787ae",
   "metadata": {},
   "source": [
    "### Write the simple dummy test to see if the model is able to do the forward pass (if it is, it means that it works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "929d977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 1000\n",
    "HIDDEN_SIZE = 128      \n",
    "INTERMEDIATE_SIZE = 256\n",
    "NUM_ATTENTION_HEADS = 8\n",
    "NUM_HIDDEN_LAYERS = 1\n",
    "MAX_POSITION_EMBEDDINGS = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33b1bf8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_model_works(Qwen2ForCausalLM, Qwen2Config):\n",
    "    try:\n",
    "        print(\"\\nInitializing minimal Qwen2Config...\")\n",
    "        # Create a configuration object with minimal parameters\n",
    "        config = Qwen2Config(\n",
    "            vocab_size=VOCAB_SIZE,\n",
    "            hidden_size=HIDDEN_SIZE,\n",
    "            intermediate_size=INTERMEDIATE_SIZE,\n",
    "            num_hidden_layers=NUM_HIDDEN_LAYERS,\n",
    "            num_attention_heads=NUM_ATTENTION_HEADS,\n",
    "            num_key_value_heads=NUM_ATTENTION_HEADS,\n",
    "            max_position_embeddings=MAX_POSITION_EMBEDDINGS,\n",
    "        )\n",
    "        print(f\"Config created: {config}\")\n",
    "\n",
    "        print(\"\\nInitializing Qwen2ForCausalLM model from config (this creates random weights)...\")\n",
    "        # Instantiate the model using the configuration.\n",
    "        # This will initialize a model with random weights according to the config.\n",
    "        model = Qwen2ForCausalLM(config)\n",
    "        model.eval() # Set the model to evaluation mode\n",
    "        print(\"Model initialized successfully.\")\n",
    "        # print(f\"Model structure:\\n{model}\") # Optional: Print model structure (can be verbose)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during model initialization: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Prepare Dummy Input\n",
    "    try:\n",
    "        print(\"\\nPreparing dummy input tensor...\")\n",
    "        # Create dummy input IDs (batch_size=1, sequence_length=5)\n",
    "        # Values must be less than vocab_size\n",
    "        input_ids = torch.randint(0, VOCAB_SIZE, (1, 5), dtype=torch.long)\n",
    "        print(f\"Dummy input_ids (shape {input_ids.shape}):\\n{input_ids}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError preparing dummy input: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Perform Forward Pass\n",
    "    try:\n",
    "        print(\"\\nPerforming forward pass...\")\n",
    "        # Perform the forward pass without calculating gradients\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids)\n",
    "\n",
    "        print(\"Forward pass completed successfully.\")\n",
    "\n",
    "        # Check the output structure (specific to CausalLM models)\n",
    "        # It should contain logits\n",
    "        if hasattr(outputs, 'logits'):\n",
    "            print(f\"Output logits shape: {outputs.logits.shape}\") # Expected: (batch_size, sequence_length, vocab_size)\n",
    "            # Verify the shape matches expectations\n",
    "            expected_shape = (input_ids.shape[0], input_ids.shape[1], VOCAB_SIZE)\n",
    "            if outputs.logits.shape == expected_shape:\n",
    "                print(f\"Test Passed: Model imported, forward pass executed, and output shape {outputs.logits.shape} is correct.\")\n",
    "            else:\n",
    "                print(f\"Test Warning: Forward pass ran, but output shape {outputs.logits.shape} does not match expected {expected_shape}.\")\n",
    "        else:\n",
    "            print(\"Test Warning: Forward pass ran, but 'logits' not found in the output object.\")\n",
    "            print(f\"Output type: {type(outputs)}\")\n",
    "            print(f\"Output keys (if dict): {outputs.keys() if isinstance(outputs, dict) else 'N/A'}\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during forward pass: {e}\")\n",
    "        print(\"Test Failed: Could not execute forward pass.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(\"\\n Test Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b69eab4",
   "metadata": {},
   "source": [
    "### Check is standard Qwen2 (without any changes works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0a9da1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/custom_transformer_python/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing minimal Qwen2Config...\n",
      "Config created: Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 256,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n",
      "\n",
      "Initializing Qwen2ForCausalLM model from config (this creates random weights)...\n",
      "Model initialized successfully.\n",
      "\n",
      "Preparing dummy input tensor...\n",
      "Dummy input_ids (shape torch.Size([1, 5])):\n",
      "tensor([[499, 432, 963, 785, 287]])\n",
      "\n",
      "Performing forward pass...\n",
      "Forward pass completed successfully.\n",
      "Output logits shape: torch.Size([1, 5, 1000])\n",
      "Test Passed: Model imported, forward pass executed, and output shape torch.Size([1, 5, 1000]) is correct.\n",
      "\n",
      " Test Finished\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2_original import Qwen2ForCausalLM as Qwen2ForCausalLM_default, Qwen2Config as Qwen2Config_default\n",
    "check_if_model_works(Qwen2ForCausalLM_default, Qwen2Config_default)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d237aefc",
   "metadata": {},
   "source": [
    "**It works**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051c083c",
   "metadata": {},
   "source": [
    "### Now check if our modified Qwen2 model works\n",
    "\n",
    "The changes that were made can be summarized as follows:\n",
    "\n",
    "**Learned Positional Encoding:**\n",
    "\n",
    "+ Removed the rotate_half and apply_rotary_pos_emb functions.\n",
    "\n",
    "+ Removed the Qwen2RotaryEmbedding class.\n",
    "\n",
    "+ In Qwen2Model.__init__, added self.learned_position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size).\n",
    "\n",
    "+ In Qwen2Model.forward, removed the call to self.rotary_emb and instead added position_embeddings = self.learned_position_embeddings(position_ids) and hidden_states = hidden_states + position_embeddings right after the input embeddings are obtained.\n",
    "\n",
    "+ Removed the position_embeddings parameter from Qwen2Attention.forward and Qwen2DecoderLayer.forward.\n",
    "\n",
    "+ Removed sin and cos from cache_kwargs in Qwen2Attention.forward.\n",
    "\n",
    "**Dynamic Tanh (DyT) instead of RMSNorm:**\n",
    "\n",
    "+ Renamed the Qwen2RMSNorm class to DyT.\n",
    "\n",
    "+ Modified the DyT.__init__ to replace self.weight with a learnable parameter self.alpha initialized to ones.\n",
    "\n",
    "+ Modified the DyT.forward method to implement the Dynamic Tanh operation: torch.tanh(self.alpha * hidden_states).\n",
    "\n",
    "+ Updated the extra_repr method in DyT.\n",
    "\n",
    "+ In Qwen2PreTrainedModel._init_weights, modified the initialization logic for the DyT module to initialize alpha to ones.\n",
    "\n",
    "+ In Qwen2DecoderLayer.__init__, replaced Qwen2RMSNorm with DyT for self.input_layernorm and self.post_attention_layernorm.\n",
    "\n",
    "+ In Qwen2Model.__init__, replaced Qwen2RMSNorm with DyT for self.norm.\n",
    "\n",
    "+ In Qwen2Model.forward, the call to self.norm now uses the DyT layer.\n",
    "\n",
    "**ReLU-Attention instead of Softmax:**\n",
    "\n",
    "+ In the eager_attention_forward function, replaced attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype) with attn_weights = nn.functional.relu(attn_weights) / seq_length to apply the ReLU function and divide by the sequence length. A comment indicates this change.\n",
    "\n",
    "**Layer-Dependent Attention Mask:**\n",
    "\n",
    "+ Modified Qwen2Model.forward:\n",
    "\n",
    "+ + The loop iterating through self.layers now includes enumerate to get the layer_idx.\n",
    "\n",
    "+ + Inside the loop, a call to a new method _create_layer_attention_mask is added to generate the attention mask specific to the current layer.\n",
    "\n",
    "+ + The generated layer_attention_mask is passed to decoder_layer.forward.\n",
    "\n",
    "+ Added _create_layer_attention_mask method to Qwen2Model:\n",
    "\n",
    "+ + This new method takes layer_idx and other necessary parameters as input.\n",
    "\n",
    "+ + The generated layer-specific mask is combined with the original attention_mask (handling padding) by adding them together (assuming min_dtype for blocked positions).\n",
    "\n",
    "+ + Includes handling for SDPA's requirement to unmask fully masked rows if applicable.\n",
    "\n",
    "**Note that we won't be able to track if the changes we made results in the increase of the model quality, since we do not have sufficient time and resources to train the entire model. However, we can at least check if the model works, and this is what we will do.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5946cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/custom_transformer_python/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing minimal Qwen2Config...\n",
      "Config created: Qwen2Config {\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 128,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 256,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 8,\n",
      "  \"num_hidden_layers\": 1,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.52.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 1000\n",
      "}\n",
      "\n",
      "\n",
      "Initializing Qwen2ForCausalLM model from config (this creates random weights)...\n",
      "Model initialized successfully.\n",
      "\n",
      "Preparing dummy input tensor...\n",
      "Dummy input_ids (shape torch.Size([1, 5])):\n",
      "tensor([[ 17, 165, 718, 182, 956]])\n",
      "\n",
      "Performing forward pass...\n",
      "Forward pass completed successfully.\n",
      "Output logits shape: torch.Size([1, 5, 1000])\n",
      "Test Passed: Model imported, forward pass executed, and output shape torch.Size([1, 5, 1000]) is correct.\n",
      "\n",
      " Test Finished\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.qwen2.modeling_qwen2 import Qwen2ForCausalLM as Qwen2ForCausalLM_MODIFIED, Qwen2Config as Qwen2Config_MODIFIED\n",
    "check_if_model_works(Qwen2ForCausalLM_MODIFIED, Qwen2Config_MODIFIED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bf9dfd",
   "metadata": {},
   "source": [
    "**it works, hence, the experiment is successful**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9235141",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "custom_transformer_python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
